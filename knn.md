# KNN

k-nearst neighbors，K近邻法

思路：如果一个样本在特征空间中，k个最相似的样本中大多数属于某一个类别，则该样本属于这个类别，一般k &lt;=20

可以做分类，也可以做决策

做分类时，多数表决法

做回归时，选择平均法

## 三要素

K值选取，距离量度的方式，分类决策的规则（一般为多数表决法）

### K值选取

没有固定经验，根据样本分布，选择一个较小的值，通过交叉检验选择一个合适的k值

较小的K值，意味着模型变得复杂，容易过拟合；

选择较大的K值，减少了泛化误差，但训练误差会增大；

### 距离量度的方式

欧式距离，曼哈顿距离，或者闵科夫斯基距离

### 优点

* 可以分类也可以回归
* 可用于非线性
* 复杂度比支持向量机低，仅为O\(n\)
* 和朴素贝叶斯相比，对数据没有假设，准确度高，对异常点不敏感
* 对于类域的交叉或重叠较多的待分样本集，KNN比其他方法适用
* 适合样本容量大的类域的自动分类，样本容量较小的类域容易产生误分

### 缺点

* 计算量大，特别是特征较多的时候
* 样本不平衡的时候，对稀有类准确率低
* KD数和球数需要大量内存
* 懒散学习方式，基本上不学习，预测速度比逻辑回归慢
* 相比决策树，KNN解释性不强



# sklearn

近邻法相关的类库都在sklearn.neighbour中

KNN分类树：KNeighborsClassifier

KNN回归树：KNeighborsRegressor

限定半径最近邻分类树：RadiusNeighborsClassifier

限定半径最近邻回归树：RadiusNeighborsRegressor

最近质心分类算法：NearestCentroid









