# KNN

k-nearst neighbors，K近邻法

思路：如果一个样本在特征空间中，k个最相似的样本中大多数属于某一个类别，则该样本属于这个类别，一般k &lt;=20

可以做分类，也可以做决策

做分类时，多数表决法

做回归时，选择平均法

## 三要素

K值选取，距离量度的方式，分类决策的规则（一般为多数表决法）

### K值选取

没有固定经验，根据样本分布，选择一个较小的值，通过交叉检验选择一个合适的k值

较小的K值，意味着模型变得复杂，容易过拟合；

选择较大的K值，减少了泛化误差，但训练误差会增大；

### 距离量度的方式

欧式距离，曼哈顿距离，或者闵科夫斯基距离

### 优点

* 可以分类也可以回归
* 可用于非线性
* 复杂度比支持向量机低，仅为O\(n\)
* 和朴素贝叶斯相比，对数据没有假设，准确度高，对异常点不敏感
* 对于类域的交叉或重叠较多的待分样本集，KNN比其他方法适用
* 适合样本容量大的类域的自动分类，样本容量较小的类域容易产生误分

### 缺点

* 计算量大，特别是特征较多的时候
* 样本不平衡的时候，对稀有类准确率低
* KD数和球数需要大量内存
* 懒散学习方式，基本上不学习，预测速度比逻辑回归慢
* 相比决策树，KNN解释性不强

# sklearn

近邻法相关的类库都在sklearn.neighbour中

KNN分类树：KNeighborsClassifier

KNN回归树：KNeighborsRegressor

限定半径最近邻分类树：RadiusNeighborsClassifier

限定半径最近邻回归树：RadiusNeighborsRegressor

最近质心分类算法：NearestCentroid（参数更简单，仅距离和特征选择阀值）

#### kneighbors\_graph

返回KNN时和每个样本最近的K个训练集样本的位置

#### radius\_neighbors\_graph

返回用限定半径最近邻法时和每个样本在限定半径内的训练集样本的位置

#### NearestNeighbors

以上两种皆可

#### n\_neighbors

k值，与样本分布有关，一般选择一个较小的k值，通过交叉检验选择一个较优的k值，默认为5。如果数据是三维以下，可以通过可视化观察来调参。

#### weights

近邻权重。如果是knn，就是k个邻近样本的权重，如果是限定半径，就是半径以内样本的权重。{uniform，distance}

{uniform}：所有权重一样，默认

{distance}，权重和距离成反比，越近的近邻，越高的权重

也可以自定义权重。

如果样本都在相对分开的组中，选择默认，如果比较乱，选择distance

### algorithm

算法，{蛮力实现brute，KD树kd_tree，_球树ball\_tree, ’auto‘}

如果样本是稀疏的，那么强制蛮力实现

如果样本特征较少，auto

如果数据量大，特征很多，kd_tree，_如果速度过慢，ball\_tree

### leaf\_size

停止建子树的叶子节点数量的阀值。

值越小，树的层次越深，默认30。依赖于样本数量，随着样本数量的增加，这个值必须增加，否则计算时间长，容易过拟合。

### Metric

距离量度，一般默认欧式距离即可。









