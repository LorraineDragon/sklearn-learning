# 决策树

既可以作为分类算法，也可以作为回归算法

### 优点

* 简单直观
* 基本不需要预处理
* 使用决策树预测的代价是O（log2（m）），m为样本数
* 既可以处理离散，也可以处理连续
* 可以处理多维问题
* 相比神经网络，在逻辑上更容易解释
* 可以交叉检验的剪枝来选择模型，从而提高泛化能力
* 异常点容错能力好，提高泛化能力

### 

### 缺点

* 容易过拟合。可以限制节点最少样本数量，限制决策树深度来改进
* 因为样本一点点变动，导致树结构的剧烈改变。可以通过集成学习方法解决
* 容易陷入局部最优。可以通过集成学习方法解决。
* 复杂的关系，比如异或，不适合。可以用神经网络
* 某特征的样本比例过大，决策树容易偏向这些特征。可以通过调节样本权重来改善。

# ID3算法

用信息增益大小来判断当前节点应该用什么特征来构建决策树

### 缺点

* 没有考虑连续特征
* 相同条件下，取值比较多的特征比取值比较少的特征的信息增益大
* 没有对缺失值进行考虑
* 没有考虑过拟合

# C4.5算法

### 缺点

* 剪枝算法有优化空间
* 二叉树会比多叉树效率高
* 只能用于分类
* 使用了熵模型，计算强度大

# CART算法

ID3和C4.5都采用了熵模型，CART使用基尼系数来替代

基尼系数代表模型的不纯度，基尼系数越小，不纯度越低，特征越好。

采用“后剪枝法”，即先生成决策树，然后产生所有可能的剪枝后的CART树，然后使用交叉验证来检验各种剪枝的效果，选择泛化能力最好的剪枝策略。

![](/assets/决策数.png)



# Sklearn决策树

分类决策树 DecisionTreeClassifier

回归决策树 DecisionTreeRegressor

参数几乎相同，但意义不全相同



### Criterion

特征选择的标准

分类：{gini（基尼系数，即CART算法），entropy（信息增益）}

回归：{mse（均方差，默认），mae（和均值差的绝对值之和）}

### Splitter

特征划分点选择标准

{best，random}

best适合样本量不大的时候

### Max\_features

划分时考虑的最大特征数

{None\(所有特征数\)，log2，sqrt/auto}

可以使整数，或者百分比

如果样本特征不多，小于50，默认None就可以；如果特征非常多，灵活使用

### Max\_depth

决策树最大深度，默认不输入。

如果模型样本量多，特征也多的情况下，推荐设置，常用的取值在10-100之间

### Min\_samples\_split

限制子树继续划分的条件，如果某节点样本数小于min\_samples\_leaf，则不再进行划分。

默认2，如果样本数量级特别大，推荐使用。如10w，设置10

### min\_samples\_leaf

限制叶子节点最少样本数，如果数目小于样本数，则会和兄弟节点一起剪枝。

默认1，可以整数，可以输入最少的样本的整数或者百分比。

如10w样本，设置5

### min\_weight\_fraction\_leaf

限制叶子节点所有样本权重和的最小值，如果小于这个值，会和兄弟结点一起被剪枝。

默认0，一般不考虑。如果样本中缺失值较多，或者分布类别偏差很大，需要引入样本权重。

### max\_leaf\_nodes

限制最大叶子节点数，可以防止过拟合，默认none。

如果特征不多，不用考虑；如果特征过多，可以加以限制，具体值可以通过交叉验证得到。

### class\_weight

样本各类别的权重，为了防止训练集某些类别的样本过多，导致训练的决策树过于偏向这些类别。

{None（默认），balanced，自定义}

### min\_impurity\_split

限制决策树增长，如果某节点的不纯度小于阀值，则不再生成子节点。

### presort

是否预排序

{true，false（默认）}

样本小时，可以选择true，加快速度。



# 其他调参注意事项

* 样本数少，特征多时，容易过拟合
* 样本少，特征很多时，先做唯独规约，如PCA、Losso、ICA（独立成分分析）
* 推荐决策树可视化，同时先限制深度，观察拟合程度后，决定是否增加深度
* 训练模型前，观察样本类别，如果分布不均匀，需要设置样本权重
* 数组使用的格式是numpy 的float32
* 如果样本是稀疏的，推荐在拟合前调用csc\_matrix稀疏化，在预测前调用csr\_matrix稀疏化。

















