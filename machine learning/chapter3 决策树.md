# 决策树

既可以作为分类算法，也可以作为回归算法



### 优点

* 简单直观
* 基本不需要预处理
* 使用决策树预测的代价是O（log2（m）），m为样本数
* 既可以处理离散，也可以处理连续
* 可以处理多维问题
* 相比神经网络，在逻辑上更容易解释
* 可以交叉检验的剪枝来选择模型，从而提高泛化能力
* 异常点容错能力好，提高泛化能力

### 

### 缺点

* 容易过拟合。可以限制节点最少样本数量，限制决策树深度来改进
* 因为样本一点点变动，导致树结构的剧烈改变。可以通过集成学习方法解决
* 容易陷入局部最优。可以通过集成学习方法解决。
* 复杂的关系，比如异或，不适合。可以用神经网络
* 某特征的样本比例过大，决策树容易偏向这些特征。可以通过调节样本权重来改善。



# ID3算法

用信息增益大小来判断当前节点应该用什么特征来构建决策树

### 缺点

* 没有考虑连续特征
* 相同条件下，取值比较多的特征比取值比较少的特征的信息增益大
* 没有对缺失值进行考虑
* 没有考虑过拟合



# C4.5算法

### 缺点

* 剪枝算法有优化空间
* 二叉树会比多叉树效率高
* 只能用于分类
* 使用了熵模型，计算强度大



# CART算法

ID3和C4.5都采用了熵模型，CART使用基尼系数来替代

基尼系数代表模型的不纯度，基尼系数越小，不纯度越低，特征越好。



采用“后剪枝法”，即先生成决策树，然后产生所有可能的剪枝后的CART树，然后使用交叉验证来检验各种剪枝的效果，选择泛化能力最好的剪枝策略。



![](/assets/决策数.png)















