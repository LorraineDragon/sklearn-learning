# 最小二乘法

The least-square method，LSM

目标函数 = Σ（观测值-理论值）^2

解法：代数解法，矩阵解法

### 局限性：

* 需要计算逆矩阵，因此需要对样本数据去除冗余
* 样本特征非常大时（超过1w），求逆耗时过长，甚至不可行。可改用迭代法或者通过主成分分析降维后使用LSM

* 拟合函数需是线性的，非线性函数可通过一些技巧转化为线性

* 当样本量m很少，小于特征数n的时候，拟合方程是欠定的，常用的优化方法都无法去拟合数据；当样本量m等于特征说n的时候，用方程组求解就可以了。当m大于n时，拟合方程是超定的，也就是我们常用与最小二乘法的场景了。

$$$$参考资料：[http://www.cnblogs.com/pinard/p/5976811.html](http://www.cnblogs.com/pinard/p/5976811.html)



# 梯度下降

Gradient Descent

### 梯度：

在微积分里面，对多元函数的参数求∂偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。

从几何意义上讲，就是函数变化增加最快的地方。或者说，沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向，梯度减少最快，也就是更加容易找到函数的最小值。

### 梯度上升与梯度下降：

通过梯度下降法来一步步的迭代求解，可以得到最小化的损失函数，和模型参数值。

反过来，如果我们需要求解损失函数的最大值，这时就需要用梯度上升法来迭代了。

梯度下降法和梯度上升法是可以互相转化的。比如我们需要求解损失函数f\(θ\)的最小值，这时我们需要用梯度下降法来迭代求解。但是实际上，我们可以反过来求解损失函数 -f\(θ\)的最大值，这时梯度上升法就派上用场了。















