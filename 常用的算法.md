# 最小二乘法

The least-square method，LSM

目标函数 = Σ（观测值-理论值）^2

解法：代数解法，矩阵解法

### 局限性：

* 需要计算逆矩阵，因此需要对样本数据去除冗余
* 样本特征非常大时（超过1w），求逆耗时过长，甚至不可行。可改用迭代法或者通过主成分分析降维后使用LSM

* 拟合函数需是线性的，非线性函数可通过一些技巧转化为线性

* 当样本量m很少，小于特征数n的时候，拟合方程是欠定的，常用的优化方法都无法去拟合数据；当样本量m等于特征说n的时候，用方程组求解就可以了。当m大于n时，拟合方程是超定的，也就是我们常用与最小二乘法的场景了。

$$$$参考资料：[http://www.cnblogs.com/pinard/p/5976811.html](http://www.cnblogs.com/pinard/p/5976811.html)

# 

# 梯度下降

Gradient Descent

### 梯度：

在微积分里面，对多元函数的参数求∂偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。

从几何意义上讲，就是函数变化增加最快的地方。或者说，沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向，梯度减少最快，也就是更加容易找到函数的最小值。

### 梯度上升与梯度下降：

通过梯度下降法来一步步的迭代求解，可以得到最小化的损失函数，和模型参数值。

反过来，如果我们需要求解损失函数的最大值，这时就需要用梯度上升法来迭代了。

梯度下降法和梯度上升法是可以互相转化的。比如我们需要求解损失函数f\(θ\)的最小值，这时我们需要用梯度下降法来迭代求解。但是实际上，我们可以反过来求解损失函数 -f\(θ\)的最大值，这时梯度上升法就派上用场了。

梯度下降不一定能找到全局最优解，有可能是一个局部最优解。

### 概念：

* #### 步长 （learning rate）

  梯度下降迭代的过程中，每一步沿梯度负方向前进的长度

* #### 特征（feature）
* #### 假设函数（hypothesis function）

  用于拟合输入样本，而使用的假设函数

* #### 损失函数（loss function）

  为了评估模型好坏，用损失函数来度量拟合的程度。损失函数极小，意味着拟合程度最好，即模型参数最优。

### 算法调优：

* #### 步长选择

  步长太大，会导致迭代过快，甚至有可能错过最优解；步长太小，迭代速度过慢，耗时长。

  可以设置不同的步长，从大到小，如果损失函数在变小，说明取值有效，否则需要增大步长。

* #### 初始选择

        初始值不同，获得的最小值也可能不同，因为梯度下降求得的只是局部最小值；（除非损失函数是凸函数）

        需要多次使用不同的初始值进行运算，选择损失函数最小化的初值

* #### 归一化

        由于样本不同特征的取值范围不一样，可能导致迭代速度过慢，可以对特征数据归一化。

         （x-期望）/标准差

### 梯度下降算法大家族：

* #### Batch Gradient Descent 批量梯度下降法 BGD

最常见的形式，更新参数的时候使用所有的样本来进行更新。

* #### Stochastic Gradient Descent 随机梯度下降法 SGD

区别是仅仅选取一个样本j来求梯度。

训练速度快，准确度（由于仅仅用一个样本决定梯度，可能不是最优解），收敛速度慢（迭代方向变化很大，不能很快收敛）

* #### Mini-Batch Gradient Descent 小批量梯度下降法 MBGD

批量和随机算法的折中，对于m个样本，取x个子样本来迭代，1&lt;x&lt;m





# 梯度下降算法与最小二乘比较

梯度下降需要选择步长，最小二乘不需要；

梯度下降是迭代求解，最小二乘是计算解析；

样本量不大，且存在解析解，最小二乘计算速度占优势，

样本量大时，由于最小二乘需要求逆，因此梯度下降更具优势；



# 牛顿法、拟牛顿法：



# 梯度下降算法与牛顿法、拟牛顿法比较：

两者都是迭代求解；

梯度下降法师梯度求解，而牛顿法、拟牛顿法是用二阶的海森矩阵的逆矩阵或伪逆矩阵求解。

相对而言，牛顿法、拟牛顿法收敛更快，但是每次迭代耗时更长。







