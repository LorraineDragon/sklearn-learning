# 最小二乘法

The least-square method，LSM

目标函数 = Σ（观测值-理论值）^2

解法：代数解法，矩阵解法

### 局限性：

* 需要计算逆矩阵，因此需要对样本数据去除冗余
* 样本特征非常大时（超过1w），求逆耗时过长，甚至不可行。可改用迭代法或者通过主成分分析降维后使用LSM

* 拟合函数需是线性的，非线性函数可通过一些技巧转化为线性

* 当样本量m很少，小于特征数n的时候，拟合方程是欠定的，常用的优化方法都无法去拟合数据；当样本量m等于特征说n的时候，用方程组求解就可以了。当m大于n时，拟合方程是超定的，也就是我们常用与最小二乘法的场景了。

$$$$参考资料：[http://www.cnblogs.com/pinard/p/5976811.html](http://www.cnblogs.com/pinard/p/5976811.html)

# 

# 梯度下降

Gradient Descent

### 梯度：

在微积分里面，对多元函数的参数求∂偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。

从几何意义上讲，就是函数变化增加最快的地方。或者说，沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向，梯度减少最快，也就是更加容易找到函数的最小值。

### 梯度上升与梯度下降：

通过梯度下降法来一步步的迭代求解，可以得到最小化的损失函数，和模型参数值。

反过来，如果我们需要求解损失函数的最大值，这时就需要用梯度上升法来迭代了。

梯度下降法和梯度上升法是可以互相转化的。比如我们需要求解损失函数f\(θ\)的最小值，这时我们需要用梯度下降法来迭代求解。但是实际上，我们可以反过来求解损失函数 -f\(θ\)的最大值，这时梯度上升法就派上用场了。

梯度下降不一定能找到全局最优解，有可能是一个局部最优解。

### 概念：

* #### 步长 （learning rate）

    梯度下降迭代的过程中，每一步沿梯度负方向前进的长度

* #### 特征（feature）

* ####  假设函数（hypothesis function）

    用于拟合输入样本，而使用的假设函数

* #### 损失函数（loss function）

    为了评估模型好坏，用损失函数来度量拟合的程度。损失函数极小，意味着拟合程度最好，即模型参数最优。



### 算法调优：

* #### 步长选择

    步长太大，会导致迭代过快，甚至有可能错过最优解；步长太小，迭代速度过慢，耗时长。

    可以设置不同的步长，从大到小，如果损失函数在变小，说明取值有效，否则需要增大步长。





































