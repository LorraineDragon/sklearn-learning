# 特征处理总论

分为特征清洗和预处理

### 特征清洗

清洗异常样本

采样（数据不均衡、样本权重）

### 特征预处理

#### 单个特征

归一化

离散化

Dummy Coding

缺失值

数据变换：log、指数、Box-Cox

#### 多个特征

降维：PCA、LDA

特征选择：filter，wrapper，embedded

filter思路：自变量和目标变量之间的关联

wrapper思路：通过目标函数来决定是否加入一个变量

embedded思路：学习器自身自动选择特征

# 数据预处理

### 无量纲化

常用：标准化（前提是服从正太分布）、区间缩放

标准化和归一化的区别：标准化是按比例缩放，归一化是把数变成（0，1）之间的小数，常用的归一化算法有：线性变换、对数转换、反余切、线性和对数结合

### 二值化

核心在于设定一个阀值。大于阀值赋值为1，小于等于赋值为0。

### 哑编码

对定性数据进行哑编码

### 缺失值

选择缺失值填充方式

### 数据变换

常用方法：基于多项式，基于指数，基于对数

# 特征选择

一般从两个方向：

* 特征是否发散
* 特征与目标的相关性

三种方法

* filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。

* wrapper：包装法，根据目标函数（通过预测效果评分），每次选择若干，或排除若干

* embendded：嵌入法，先使用机器学习算法或者模型进行训练，得到各个特征的权重系数，根据系数从大大小选择

## Filter

### 方差选择

计算各个特征的方差，选择大于阀值的特征

### 相关系数法

计算相关系数及显著性水平p值

### 卡方检验

检验自变量对因变量的相关性，检验是否独立，超过阀值，表示不独立，存在相关性

### 互信息法

用来评价一个事件出现对于另一个事件出现贡献的信息量

\*\* 卡方和互信息都属于贪心算法，没有考虑已选特征和待选特征之间的相关性

## wrapper

### 递归特征消除法

使用一个基模型进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行新的训练

## Embedded

### 基于惩罚项的特征选择法

使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。

L1惩罚项降维的原理在于保留多个对目标值具有同等相关性的特征中的一个，所以没选到的特征不代表不重要。故，可结合L2惩罚项来优化。

### 基于树模型的特征选择法

树模型中GBDT也可用来作为基模型进行特征选择

# 降维

解决特征矩阵过大，导致计算量大，训练时间长的问题。

常用的降维方法：基于L1惩罚项，主成分分析（PCA），线性判别分析（LDA）

#### PCA和LDA的区别

PCA，无监督

LDA，有监督

\*注意，这里的LDA区别于nlp的LDA，这里指的是Linear Discriminant Analysis（线性判别分析）





